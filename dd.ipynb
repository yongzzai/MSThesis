{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca6690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 차원: torch.Size([32, 20, 100])\n",
      "출력 차원: torch.Size([32, 20, 512])\n",
      "은닉 상태 차원: torch.Size([2, 32, 256])\n",
      "\n",
      "=== 차원 변화 분석 ===\n",
      "입력: (batch_size=32, seq_length=20, input_size=100)\n",
      "출력: (batch_size=32, seq_length=20, hidden_size*2=512)\n",
      "은닉상태: (num_layers*2=2, batch_size=32, hidden_size=256)\n",
      "\n",
      "※ Bidirectional이므로 출력 차원은 hidden_size의 2배가 됩니다.\n",
      "=== Hidden vs Output 차이점 ===\n",
      "Output 형태: torch.Size([32, 20, 512])\n",
      "Hidden 형태: torch.Size([2, 32, 256])\n",
      "\n",
      "=== 각각의 의미 ===\n",
      "Output: 모든 시간 스텝에서의 은닉 상태 (forward + backward 결합)\n",
      "Hidden: 마지막 시간 스텝의 은닉 상태 (forward와 backward 분리)\n",
      "\n",
      "Forward hidden: torch.Size([32, 256])\n",
      "Backward hidden: torch.Size([32, 256])\n",
      "\n",
      "Last output: torch.Size([32, 512])\n",
      "Last forward from output: torch.Size([32, 256])\n",
      "Last backward from output: torch.Size([32, 256])\n",
      "\n",
      "=== Decoder에서 사용 권장사항 ===\n",
      "1. Sequence-to-sequence: hidden state 사용 (context vector로)\n",
      "2. Attention mechanism: output 사용 (모든 시간 스텝 정보)\n",
      "3. Classification: output의 마지막 또는 평균 사용\n",
      "\n",
      "Context vector for decoder: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Bi-GRU 모델 정의\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.bigru = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, hidden = self.bigru(x)\n",
    "        return output, hidden\n",
    "\n",
    "# 차원 변화 확인\n",
    "input_size = 100  # 입력 특성 차원\n",
    "hidden_size = 256  # 은닉 상태 차원\n",
    "seq_length = 20   # 시퀀스 길이\n",
    "batch_size = 32   # 배치 크기\n",
    "\n",
    "# 모델 생성\n",
    "model = BiGRU(input_size, hidden_size)\n",
    "\n",
    "# 더미 입력 데이터 생성\n",
    "input_data = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "print(f\"입력 차원: {input_data.shape}\")\n",
    "\n",
    "# Bi-GRU를 통과시켜 representation 추출\n",
    "with torch.no_grad():\n",
    "    output, hidden = model(input_data)\n",
    "\n",
    "print(f\"출력 차원: {output.shape}\")\n",
    "print(f\"은닉 상태 차원: {hidden.shape}\")\n",
    "\n",
    "# 차원 변화 설명\n",
    "print(\"\\n=== 차원 변화 분석 ===\")\n",
    "print(f\"입력: (batch_size={batch_size}, seq_length={seq_length}, input_size={input_size})\")\n",
    "print(f\"출력: (batch_size={batch_size}, seq_length={seq_length}, hidden_size*2={hidden_size*2})\")\n",
    "print(f\"은닉상태: (num_layers*2={2}, batch_size={batch_size}, hidden_size={hidden_size})\")\n",
    "print(\"\\n※ Bidirectional이므로 출력 차원은 hidden_size의 2배가 됩니다.\")\n",
    "\n",
    "# hidden과 output의 차이점 분석\n",
    "print(\"=== Hidden vs Output 차이점 ===\")\n",
    "print(f\"Output 형태: {output.shape}\")  # (batch_size, seq_length, hidden_size*2)\n",
    "print(f\"Hidden 형태: {hidden.shape}\")  # (num_layers*2, batch_size, hidden_size)\n",
    "\n",
    "print(\"\\n=== 각각의 의미 ===\")\n",
    "print(\"Output: 모든 시간 스텝에서의 은닉 상태 (forward + backward 결합)\")\n",
    "print(\"Hidden: 마지막 시간 스텝의 은닉 상태 (forward와 backward 분리)\")\n",
    "\n",
    "# Hidden state 분석\n",
    "forward_hidden = hidden[0]  # Forward GRU의 마지막 은닉 상태\n",
    "backward_hidden = hidden[1]  # Backward GRU의 마지막 은닉 상태\n",
    "\n",
    "print(f\"\\nForward hidden: {forward_hidden.shape}\")\n",
    "print(f\"Backward hidden: {backward_hidden.shape}\")\n",
    "\n",
    "# Output의 마지막 시간 스텝과 Hidden 비교\n",
    "last_output = output[:, -1, :]  # 마지막 시간 스텝의 output\n",
    "print(f\"\\nLast output: {last_output.shape}\")\n",
    "\n",
    "# Forward와 Backward 분리\n",
    "last_forward = output[:, -1, :hidden_size]   # Forward 부분\n",
    "last_backward = output[:, -1, hidden_size:]  # Backward 부분\n",
    "\n",
    "print(f\"Last forward from output: {last_forward.shape}\")\n",
    "print(f\"Last backward from output: {last_backward.shape}\")\n",
    "\n",
    "# Decoder에서 사용할 때의 권장사항\n",
    "print(\"\\n=== Decoder에서 사용 권장사항 ===\")\n",
    "print(\"1. Sequence-to-sequence: hidden state 사용 (context vector로)\")\n",
    "print(\"2. Attention mechanism: output 사용 (모든 시간 스텝 정보)\")\n",
    "print(\"3. Classification: output의 마지막 또는 평균 사용\")\n",
    "\n",
    "# Context vector 생성 예시\n",
    "context_vector = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "print(f\"\\nContext vector for decoder: {context_vector.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f64689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
