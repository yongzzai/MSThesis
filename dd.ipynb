{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca6690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_size: 32, \n",
      "Sequence Length: 20\n",
      "Hidden: 256\n",
      "\n",
      "GRU----------------------\n",
      "입력 차원: torch.Size([32, 20, 100])\n",
      "출력 차원: torch.Size([32, 20, 256])\n",
      "은닉 상태 차원: torch.Size([4, 32, 256])\n",
      "마지막 은닉 상태 차원: torch.Size([32, 256])\n",
      "\n",
      "Bi-GRU-------------------\n",
      "입력 차원: torch.Size([32, 20, 100])\n",
      "출력 차원: torch.Size([32, 20, 512])\n",
      "은닉 상태 차원: torch.Size([8, 32, 256])\n",
      "마지막 은닉 상태 차원: torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Bi-GRU 모델 정의\n",
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=4):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.bigru = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, hidden = self.bigru(x)\n",
    "        return output, hidden\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=4):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "# 차원 변화 확인\n",
    "input_size = 100  # 입력 특성 차원\n",
    "hidden_size = 256  # 은닉 상태 차원\n",
    "seq_length = 20   # 시퀀스 길이\n",
    "batch_size = 32   # 배치 크기\n",
    "\n",
    "# 모델 생성\n",
    "model0 = GRU(input_size, hidden_size)\n",
    "model = BiGRU(input_size, hidden_size)\n",
    "\n",
    "# 더미 입력 데이터 생성\n",
    "input_data = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "print(f\"Batch_size: {batch_size}, \\nSequence Length: {seq_length}\")\n",
    "print(f\"Hidden: {hidden_size}\")\n",
    "\n",
    "# Bi-GRU를 통과시켜 representation 추출\n",
    "with torch.no_grad():\n",
    "    output0, hidden0 = model0(input_data)\n",
    "    output, hidden = model(input_data)\n",
    "\n",
    "fbhidden0 = hidden0[-1]  # GRU의 은닉 상태를 합칩니다.\n",
    "fbhidden = torch.cat([hidden[-2], hidden[-1]], dim=-1)  # 양방향 GRU의 은닉 상태를 합칩니다.\n",
    "\n",
    "print()\n",
    "print(\"GRU----------------------\")\n",
    "print(f\"입력 차원: {input_data.shape}\")\n",
    "print(f\"출력 차원: {output0.shape}\")\n",
    "print(f\"은닉 상태 차원: {hidden0.shape}\")\n",
    "print(f\"마지막 은닉 상태 차원: {fbhidden0.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Bi-GRU-------------------\")\n",
    "print(f\"입력 차원: {input_data.shape}\")\n",
    "print(f\"출력 차원: {output.shape}\")\n",
    "print(f\"은닉 상태 차원: {hidden.shape}\")\n",
    "print(f\"마지막 은닉 상태 차원: {fbhidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba06ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Shape: torch.Size([3, 5, 3])\n",
      "After Embedding: torch.Size([3, 5, 128])\n",
      "After GRU Output: torch.Size([3, 5, 128])\n",
      "After GRU Hidden: torch.Size([4, 3, 64])\n",
      "After GRU Fwd: torch.Size([3, 64])\n",
      "After GRU Bwd: torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "class EventSeqEncoder(nn.Module):\n",
    "    def __init__(self, attr_dims:list, hidden_dim:int, num_layers:int):\n",
    "        super(EventSeqEncoder, self).__init__()\n",
    "\n",
    "        self.attr_embs = nn.ModuleList([nn.Embedding(dim, hidden_dim) for dim in attr_dims[1:]])\n",
    "        self.gru = nn.GRU(input_size=hidden_dim*len(attr_dims[1:]),\n",
    "                          hidden_size=hidden_dim,\n",
    "                          num_layers=num_layers, \n",
    "                          dropout=0.3, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, seq):\n",
    "\n",
    "        print(\"Before Shape:\", seq.shape)\n",
    "\n",
    "        # seq: (Batch_size, Seq_len, Attr_num)\n",
    "\n",
    "        embedded = []\n",
    "        for i, m in enumerate(self.attr_embs):\n",
    "            embedded.append(m(seq[:, :, i]))\n",
    "\n",
    "        seq = torch.cat(embedded, dim=2)  # (Batch_size, Seq_len, hidden_dim * (Attr_num-1))\n",
    "\n",
    "        print(\"After Embedding:\", seq.shape)\n",
    "\n",
    "        self.gru.flatten_parameters()\n",
    "        output, hidden = self.gru(seq)\n",
    "\n",
    "        fwd, bwd = hidden[-2, :, :], hidden[-1, :, :]\n",
    "\n",
    "        print(\"After GRU Output:\", output.shape)\n",
    "        print(\"After GRU Hidden:\", hidden.shape)\n",
    "        print(\"After GRU Fwd:\", fwd.shape)\n",
    "        print(\"After GRU Bwd:\", bwd.shape)\n",
    "\n",
    "# Sample Input\n",
    "attr_dims = [10, 20, 30]  # 예시로 각 속성의 차원\n",
    "hidden_dim = 64  # 은닉 상태 차원\n",
    "num_layers = 2  # GRU 레이어 수\n",
    "seq_len = 5  # 시퀀스 길이\n",
    "batch_size = 3  # 배치 크기\n",
    "\n",
    "seq = torch.randint(0, 10, (batch_size, seq_len, len(attr_dims)))  # 예시 시퀀스 데이터\n",
    "encoder = EventSeqEncoder(attr_dims, hidden_dim, num_layers)\n",
    "encoder(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f64689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20])\n",
      "torch.Size([10, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(10, 20)  # 예시로 랜덤 텐서 생성\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "res = sm(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e3e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, attr_dims:list, hidden_dim:int):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "\n",
    "        self.acts_embedding = nn.Embedding(attr_dims[0], hidden_dim)\n",
    "        self.conv1 = GATConv(in_channels=hidden_dim,\n",
    "                            out_channels=hidden_dim,\n",
    "                            heads=2, dropout=0.3)\n",
    "        self.act1 = nn.ELU()\n",
    "        self.do1 = nn.Dropout(p=0.3)    \n",
    "\n",
    "        self.conv2 = GATConv(in_channels=hidden_dim * 2,\n",
    "                            out_channels=hidden_dim,\n",
    "                            heads=2, dropout=0.3)\n",
    "        self.act2 = nn.ELU()\n",
    "        self.do2 = nn.Dropout(p=0.3)\n",
    "        self.projection = nn.Linear(hidden_dim * 2, hidden_dim)     # Shape(A, H)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        x = self.acts_embedding(x)\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.act1(h)\n",
    "        h = self.do1(h)\n",
    "\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self.act2(h)\n",
    "        h = self.do2(h)\n",
    "\n",
    "        h = self.projection(h)              # Shape(A, H)\n",
    "        z = global_mean_pool(h, batch_idx)  # Shape(B, H)\n",
    "        return h, z\n",
    "\n",
    "\n",
    "class EventSeqEncoder(nn.Module):\n",
    "    def __init__(self, attr_dims:list, hidden_dim:int, num_layers:int):\n",
    "        super(EventSeqEncoder, self).__init__()\n",
    "\n",
    "        self.attr_embs = nn.ModuleList([nn.Embedding(dim, hidden_dim) for dim in attr_dims[1:]])\n",
    "        self.gru = nn.GRU(input_size=hidden_dim*len(attr_dims[1:]),\n",
    "                          hidden_size=hidden_dim,\n",
    "                          num_layers=num_layers, \n",
    "                          dropout=0.3, batch_first=True, bidirectional=True)\n",
    "\n",
    "\n",
    "    def forward(self, seq):\n",
    "        self.gru.flatten_parameters()\n",
    "        # seq: (Batch_size, Seq_len, Attr_num)\n",
    "        embs = []\n",
    "        for i, m in enumerate(self.attr_embs):\n",
    "            embs.append(m(seq[:, :, i]))\n",
    "        \n",
    "        seq_embs = torch.cat(embs, dim=2)\n",
    "        # seq_embs: (batch_size, seq_len, num_attrs * hidden_dim)\n",
    "\n",
    "        out, hidden = self.gru(seq_embs)\n",
    "\n",
    "        fwd, bwd = hidden[-2, :, :], hidden[-1, :, :]   # Shape(batch_size, hidden)\n",
    "        return out, fwd, bwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174abb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphEncoder output shapes:\n",
      "h (node embeddings): torch.Size([6, 16])\n",
      "z (graph embeddings): torch.Size([2, 16])\n",
      "\n",
      "EventSeqEncoder output shapes:\n",
      "out (all hidden states): torch.Size([4, 5, 32])\n",
      "fwd (forward last hidden): torch.Size([4, 16])\n",
      "bwd (backward last hidden): torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "# 예시 입력 텐서 생성 및 모델 입출력 테스트\n",
    "\n",
    "# GraphEncoder 테스트용 입력\n",
    "# 노드 feature: action id (정수), edge_index: (2, num_edges), batch_idx: (num_nodes,)\n",
    "num_nodes = 6\n",
    "num_edges = 8\n",
    "action_ids = torch.randint(0, attr_dims[0], (num_nodes,))  # (num_nodes,)\n",
    "edge_index = torch.randint(0, num_nodes, (2, num_edges))    # (2, num_edges)\n",
    "batch_idx = torch.randint(0, 2, (num_nodes,))               # (num_nodes,)\n",
    "\n",
    "graph_encoder = GraphEncoder(attr_dims, hidden_dim)\n",
    "h, z = graph_encoder(action_ids, edge_index, batch_idx)\n",
    "print(\"GraphEncoder output shapes:\")\n",
    "print(\"h (node embeddings):\", h.shape)  # (num_nodes, hidden_dim)\n",
    "print(\"z (graph embeddings):\", z.shape) # (num_graphs, hidden_dim)\n",
    "\n",
    "# EventSeqEncoder 테스트용 입력\n",
    "# 시퀀스: (batch_size, seq_len, attr_num)\n",
    "batch_size = 4\n",
    "seq_len = 5\n",
    "attr_num = len(attr_dims)\n",
    "seq = torch.randint(0, 10, (batch_size, seq_len, attr_num))  # (batch_size, seq_len, attr_num)\n",
    "\n",
    "event_seq_encoder = EventSeqEncoder(attr_dims, hidden_dim, num_layers)\n",
    "out, fwd, bwd = event_seq_encoder(seq)\n",
    "print(\"\\nEventSeqEncoder output shapes:\")\n",
    "print(\"out (all hidden states):\", out.shape)   # (batch_size, seq_len, hidden_dim*2)\n",
    "print(\"fwd (forward last hidden):\", fwd.shape) # (batch_size, hidden_dim)\n",
    "print(\"bwd (backward last hidden):\", bwd.shape)# (batch_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716da8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
